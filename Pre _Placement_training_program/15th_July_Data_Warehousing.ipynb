{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGEsJQP26wdC"
      },
      "outputs": [],
      "source": [
        "\n",
        "TOPIC: Data Warehousing Fundamentals\n",
        "1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
        "\n",
        "Fact Table: Sales Fact Table\n",
        "Dimension Tables: Product Dimension, Customer Dimension, Time Dimension\n",
        "Schema:\n",
        "\n",
        "Sales Fact Table:\n",
        "\n",
        "sale_id: INT (Primary Key)\n",
        "product_id: INT (Foreign Key to Product Dimension)\n",
        "customer_id: INT (Foreign Key to Customer Dimension)\n",
        "date_id: INT (Foreign Key to Time Dimension)\n",
        "sales_amount: DECIMAL\n",
        "Product Dimension:\n",
        "\n",
        "product_id: INT (Primary Key)\n",
        "product_name: VARCHAR\n",
        "category: VARCHAR\n",
        "price: DECIMAL\n",
        "Customer Dimension:\n",
        "\n",
        "customer_id: INT (Primary Key)\n",
        "customer_name: VARCHAR\n",
        "address: VARCHAR\n",
        "email: VARCHAR\n",
        "Time Dimension:\n",
        "\n",
        "date_id: INT (Primary Key)\n",
        "date: DATE\n",
        "day: INT\n",
        "month: INT\n",
        "year: INT\n",
        "quarter: INT\n",
        "\n",
        "SQL to Create Tables (Using PostgreSQL):\n",
        "\n",
        "-- Product Dimension Table\n",
        "CREATE TABLE product_dim (\n",
        "    product_id SERIAL PRIMARY KEY,\n",
        "    product_name VARCHAR(100),\n",
        "    category VARCHAR(50),\n",
        "    price DECIMAL(10, 2)\n",
        ");\n",
        "\n",
        "-- Customer Dimension Table\n",
        "CREATE TABLE customer_dim (\n",
        "    customer_id SERIAL PRIMARY KEY,\n",
        "    customer_name VARCHAR(100),\n",
        "    address VARCHAR(200),\n",
        "    email VARCHAR(100)\n",
        ");\n",
        "\n",
        "-- Time Dimension Table\n",
        "CREATE TABLE time_dim (\n",
        "    date_id SERIAL PRIMARY KEY,\n",
        "    date DATE,\n",
        "    day INT,\n",
        "    month INT,\n",
        "    year INT,\n",
        "    quarter INT\n",
        ");\n",
        "\n",
        "-- Sales Fact Table\n",
        "CREATE TABLE sales_fact (\n",
        "    sale_id SERIAL PRIMARY KEY,\n",
        "    product_id INT REFERENCES product_dim(product_id),\n",
        "    customer_id INT REFERENCES customer_dim(customer_id),\n",
        "    date_id INT REFERENCES time_dim(date_id),\n",
        "    sales_amount DECIMAL(10, 2)\n",
        ");\n",
        "\n",
        "   2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
        "\n",
        "   -- Insert data into product_dim\n",
        "INSERT INTO product_dim (product_name, category, price)\n",
        "VALUES ('Laptop', 'Electronics', 1000.00),\n",
        "       ('Shoes', 'Apparel', 50.00);\n",
        "\n",
        "-- Insert data into customer_dim\n",
        "INSERT INTO customer_dim (customer_name, address, email)\n",
        "VALUES ('John Doe', '123 Main St', 'john@example.com'),\n",
        "       ('Jane Smith', '456 Oak St', 'jane@example.com');\n",
        "\n",
        "-- Insert data into time_dim\n",
        "INSERT INTO time_dim (date, day, month, year, quarter)\n",
        "VALUES ('2023-08-01', 1, 8, 2023, 3),\n",
        "       ('2023-08-02', 2, 8, 2023, 3);\n",
        "\n",
        "-- Insert data into sales_fact\n",
        "INSERT INTO sales_fact (product_id, customer_id, date_id, sales_amount)\n",
        "VALUES (1, 1, 1, 1000.00),\n",
        "       (2, 2, 2, 50.00);\n",
        "\n",
        "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n",
        "\n",
        "Total Sales by Product:\n",
        "\n",
        "SELECT p.product_name, SUM(s.sales_amount) AS total_sales\n",
        "FROM sales_fact s\n",
        "JOIN product_dim p ON s.product_id = p.product_id\n",
        "GROUP BY p.product_name;\n",
        "\n",
        "Total Sales by Customer:\n",
        "\n",
        "SELECT c.customer_name, SUM(s.sales_amount) AS total_sales\n",
        "FROM sales_fact s\n",
        "JOIN customer_dim c ON s.customer_id = c.customer_id\n",
        "GROUP BY c.customer_name;\n",
        "\n",
        "Sales in August 2023:\n",
        "SELECT t.month, t.year, SUM(s.sales_amount) AS total_sales\n",
        "FROM sales_fact s\n",
        "JOIN time_dim t ON s.date_id = t.date_id\n",
        "WHERE t.month = 8 AND t.year = 2023\n",
        "GROUP BY t.month, t.year;\n",
        "\n",
        "TOPIC: ETL and Data Integration\n",
        "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
        "  TOPIC: ETL and Data Integration\n",
        "\n",
        "  import pandas as pd\n",
        "import psycopg2\n",
        "\n",
        "def extract_data(csv_file):\n",
        "    # Extract data from CSV file\n",
        "    data = pd.read_csv(csv_file)\n",
        "    return data\n",
        "\n",
        "def transform_data(data):\n",
        "    # Apply transformations like removing duplicates, handling missing values\n",
        "    data = data.drop_duplicates()\n",
        "    data = data.dropna()\n",
        "    return data\n",
        "\n",
        "def load_data(data, connection):\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Insert data into the sales_fact table\n",
        "    for index, row in data.iterrows():\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO sales_fact (product_id, customer_id, date_id, sales_amount)\n",
        "            VALUES (%s, %s, %s, %s)\n",
        "        \"\"\", (row['product_id'], row['customer_id'], row['date_id'], row['sales_amount']))\n",
        "\n",
        "    connection.commit()\n",
        "\n",
        "def etl_process(csv_file, connection):\n",
        "    data = extract_data(csv_file)\n",
        "    transformed_data = transform_data(data)\n",
        "    load_data(transformed_data, connection)\n",
        "\n",
        "# Usage\n",
        "connection = psycopg2.connect(\n",
        "    host=\"localhost\",\n",
        "    database=\"retail_db\",\n",
        "    user=\"user\",\n",
        "    password=\"password\"\n",
        ")\n",
        "\n",
        "etl_process(\"sales_data.csv\", connection)\n",
        "connection.close()\n",
        "\n",
        "1. Design an ETL Process (Python)\n",
        "\n",
        "Extract: Extract data from a CSV file.\n",
        "Transform: Apply business rules like converting data formats, filtering invalid data.\n",
        "Load: Load data into the data warehouse.\n",
        "ETL Design:\n",
        "\n",
        "Source: CSV file containing product sales data.\n",
        "Transformations: Convert data types, remove duplicates, validate fields.\n",
        "Load: Insert data into the sales_fact table.\n",
        "\n",
        "\n",
        "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n",
        "  import pandas as pd\n",
        "import psycopg2\n",
        "\n",
        "def extract_data(csv_file):\n",
        "    # Extract data from CSV file\n",
        "    data = pd.read_csv(csv_file)\n",
        "    return data\n",
        "\n",
        "def transform_data(data):\n",
        "    # Apply transformations like removing duplicates, handling missing values\n",
        "    data = data.drop_duplicates()\n",
        "    data = data.dropna()\n",
        "    return data\n",
        "\n",
        "def load_data(data, connection):\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Insert data into the sales_fact table\n",
        "    for index, row in data.iterrows():\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO sales_fact (product_id, customer_id, date_id, sales_amount)\n",
        "            VALUES (%s, %s, %s, %s)\n",
        "        \"\"\", (row['product_id'], row['customer_id'], row['date_id'], row['sales_amount']))\n",
        "\n",
        "    connection.commit()\n",
        "\n",
        "def etl_process(csv_file, connection):\n",
        "    data = extract_data(csv_file)\n",
        "    transformed_data = transform_data(data)\n",
        "    load_data(transformed_data, connection)\n",
        "\n",
        "# Usage\n",
        "connection = psycopg2.connect(\n",
        "    host=\"localhost\",\n",
        "    database=\"retail_db\",\n",
        "    user=\"user\",\n",
        "    password=\"password\"\n",
        ")\n",
        "\n",
        "etl_process(\"sales_data.csv\", connection)\n",
        "connection.close()\n",
        "\n",
        "TOPIC: Dimensional Modeling and Schemas\n",
        "\n",
        "1. Design a Star Schema for University Database\n",
        "\n",
        "Fact Table: Enrollment Fact\n",
        "\n",
        "enrollment_id: INT\n",
        "student_id: INT (FK)\n",
        "course_id: INT (FK)\n",
        "date_id: INT (FK)\n",
        "grade: CHAR(2)\n",
        "Student Dimension:\n",
        "\n",
        "student_id: INT (Primary Key)\n",
        "student_name: VARCHAR\n",
        "major: VARCHAR\n",
        "Course Dimension:\n",
        "\n",
        "course_id: INT (Primary Key)\n",
        "course_name: VARCHAR\n",
        "department: VARCHAR\n",
        "Time Dimension:\n",
        "\n",
        "date_id: INT (Primary Key)\n",
        "semester: VARCHAR\n",
        "year: INT\n",
        "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
        "   -- Fact Table: Enrollment\n",
        "CREATE TABLE enrollment_fact (\n",
        "    enrollment_id SERIAL PRIMARY KEY,\n",
        "    student_id INT REFERENCES student_dim(student_id),\n",
        "    course_id INT REFERENCES course_dim(course_id),\n",
        "    date_id INT REFERENCES time_dim(date_id),\n",
        "    grade CHAR(2)\n",
        ");\n",
        "\n",
        "-- Student Dimension\n",
        "CREATE TABLE student_dim (\n",
        "    student_id SERIAL PRIMARY KEY,\n",
        "    student_name VARCHAR(100),\n",
        "    major VARCHAR(50)\n",
        ");\n",
        "\n",
        "-- Course Dimension\n",
        "CREATE TABLE course_dim (\n",
        "    course_id SERIAL PRIMARY KEY,\n",
        "    course_name VARCHAR(100),\n",
        "    department VARCHAR(50)\n",
        ");\n",
        "\n",
        "-- Time Dimension\n",
        "CREATE TABLE time_dim (\n",
        "    date_id SERIAL PRIMARY KEY,\n",
        "    semester VARCHAR(20),\n",
        "    year INT\n",
        ");\n",
        "\n",
        "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n",
        "\n",
        "   -- Fact Table: Enrollment\n",
        "CREATE TABLE enrollment_fact (\n",
        "    enrollment_id SERIAL PRIMARY KEY,\n",
        "    student_id INT REFERENCES student_dim(student_id),\n",
        "    course_id INT REFERENCES course_dim(course_id),\n",
        "    date_id INT REFERENCES time_dim(date_id),\n",
        "    grade CHAR(2)\n",
        ");\n",
        "\n",
        "-- Student Dimension\n",
        "CREATE TABLE student_dim (\n",
        "    student_id SERIAL PRIMARY KEY,\n",
        "    student_name VARCHAR(100),\n",
        "    major VARCHAR(50)\n",
        ");\n",
        "\n",
        "-- Course Dimension\n",
        "CREATE TABLE course_dim (\n",
        "    course_id SERIAL PRIMARY KEY,\n",
        "    course_name VARCHAR(100),\n",
        "    department VARCHAR(50)\n",
        ");\n",
        "\n",
        "-- Time Dimension\n",
        "CREATE TABLE time_dim (\n",
        "    date_id SERIAL PRIMARY KEY,\n",
        "    semester VARCHAR(20),\n",
        "    year INT\n",
        ");\n",
        "\n",
        "\n",
        "TOPIC: Performance Optimization and Querying\n",
        "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
        "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
        "\n",
        ". Python Script for Optimized Data Loading\n",
        "\n",
        "Batch Processing: Load data in bulk instead of row-by-row.\n",
        "Multi-threading/Multiprocessing: Parallelize the loading process.\n",
        "\n",
        "import psycopg2\n",
        "import threading\n",
        "import pandas as pd\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Batch processing\n",
        "def load_batch(data_batch, connection):\n",
        "    cursor = connection.cursor()\n",
        "    for row in data_batch:\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO sales_fact (product_id, customer_id, date_id, sales_amount)\n",
        "            VALUES (%s, %s, %s, %s)\n",
        "        \"\"\", (row['product_id'], row['customer_id'], row['date_id'], row['sales_amount']))\n",
        "    connection.commit()\n",
        "\n",
        "# Parallel processing using multiprocessing\n",
        "def parallel_load(data, connection):\n",
        "    num_workers = 4\n",
        "    pool = Pool(num_workers)\n",
        "\n",
        "    # Split data into chunks for parallel loading\n",
        "    chunks = [data[i::num_workers] for i in range(num_workers)]\n",
        "    pool.starmap(load_batch, [(chunk, connection) for chunk in chunks])\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "# Measure performance\n",
        "def measure_performance(csv_file, connection):\n",
        "    data = pd.read_csv(csv_file).to_dict(orient='records')\n",
        "\n",
        "    # Load data without optimization\n",
        "    start_time = time.time()\n",
        "    load_batch(data, connection)\n",
        "    print(\"Time without optimization:\", time.time() - start_time)\n",
        "\n",
        "    # Load data with parallel processing\n",
        "    start_time = time.time()\n",
        "    parallel_load(data, connection)\n",
        "    print(\"Time with optimization:\", time.time() - start_time)\n",
        "\n",
        "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
        "      import psycopg2\n",
        "import threading\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Function to load a batch of data\n",
        "def load_batch(data_batch, connection_params):\n",
        "    try:\n",
        "        connection = psycopg2.connect(**connection_params)\n",
        "        cursor = connection.cursor()\n",
        "        for row in data_batch:\n",
        "            cursor.execute(\"\"\"\n",
        "                INSERT INTO sales_fact (product_id, customer_id, date_id, sales_amount)\n",
        "                VALUES (%s, %s, %s, %s)\n",
        "            \"\"\", (row['product_id'], row['customer_id'], row['date_id'], row['sales_amount']))\n",
        "        connection.commit()\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error while loading data batch: {e}\")\n",
        "\n",
        "# Multi-threaded data loading\n",
        "def multi_threaded_load(data, connection_params, num_threads=4):\n",
        "    threads = []\n",
        "    # Split data into chunks for each thread\n",
        "    chunk_size = len(data) // num_threads\n",
        "    for i in range(num_threads):\n",
        "        start_index = i * chunk_size\n",
        "        if i == num_threads - 1:\n",
        "            data_chunk = data[start_index:]  # Last chunk takes the remaining data\n",
        "        else:\n",
        "            data_chunk = data[start_index:start_index + chunk_size]\n",
        "\n",
        "        # Create a thread for each chunk\n",
        "        thread = threading.Thread(target=load_batch, args=(data_chunk, connection_params))\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    connection_params = {\n",
        "        'host': 'localhost',\n",
        "        'database': 'retail_db',\n",
        "        'user': 'your_user',\n",
        "        'password': 'your_password'\n",
        "    }\n",
        "    data = pd.read_csv('sales_data.csv').to_dict(orient='records')\n",
        "\n",
        "    # Measure time for multi-threaded loading\n",
        "    start_time = time.time()\n",
        "    multi_threaded_load(data, connection_params, num_threads=4)\n",
        "    print(f\"Time taken with multi-threading:\n",
        "\n",
        "import psycopg2\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Function to load a batch of data (for multiprocessing)\n",
        "def load_batch_mp(data_batch, connection_params):\n",
        "    try:\n",
        "        connection = psycopg2.connect(**connection_params)\n",
        "        cursor = connection.cursor()\n",
        "        for row in data_batch:\n",
        "            cursor.execute(\"\"\"\n",
        "                INSERT INTO sales_fact (product_id, customer_id, date_id, sales_amount)\n",
        "                VALUES (%s, %s, %s, %s)\n",
        "            \"\"\", (row['product_id'], row['customer_id'], row['date_id'], row['sales_amount']))\n",
        "        connection.commit()\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error while loading data batch: {e}\")\n",
        "\n",
        "# Multiprocessing data loading\n",
        "def multiprocessing_load(data, connection_params, num_processes=4):\n",
        "    pool = mp.Pool(num_processes)\n",
        "    chunk_size = len(data) // num_processes\n",
        "    chunks = [data[i * chunk_size: (i + 1) * chunk_size] for i in range(num_processes)]\n",
        "\n",
        "    # Pass the data chunks and connection params to the multiprocessing pool\n",
        "    pool.starmap(load_batch_mp, [(chunk, connection_params) for chunk in chunks])\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    connection_params = {\n",
        "        'host': 'localhost',\n",
        "        'database': 'retail_db',\n",
        "        'user': 'your_user',\n",
        "        'password': 'your_password'\n",
        "    }\n",
        "    data = pd.read_csv('sales_data.csv').to_dict(orient='records')\n",
        "\n",
        "    # Measure time for multiprocessing loading\n",
        "    start_time = time.time()\n",
        "    multiprocessing_load(data, connection_params, num_processes=4)\n",
        "    print(f\"Time taken with multiprocessing: {time.time() - start_time} seconds\")\n",
        "\n",
        "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n",
        "\n",
        "      3. Measure the Time Before and After Optimizations\n",
        "To compare the time taken with and without optimizations:\n",
        "\n",
        "Without Optimization (single-threaded approach):\n",
        "if __name__ == \"__main__\":\n",
        "    connection_params = {\n",
        "        'host': 'localhost',\n",
        "        'database': 'retail_db',\n",
        "        'user': 'your_user',\n",
        "        'password': 'your_password'\n",
        "    }\n",
        "    data = pd.read_csv('sales_data.csv').to_dict(orient='records')\n",
        "\n",
        "    # Single-threaded loading\n",
        "    start_time = time.time()\n",
        "    load_batch(data, connection_params)\n",
        "    print(f\"Time taken without optimization: {time.time() - start_time} seconds\")\n",
        "\n",
        "With Multi-threading:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    multi_threaded_load(data, connection_params, num_threads=4)\n",
        "    print(f\"Time taken with multi-threading: {time.time() - start_time} seconds\")\n",
        "\n",
        "With Multiprocessing:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    multiprocessing_load(data, connection_params, num_processes=4)\n",
        "    print(f\"Time taken with multiprocessing: {time.time() - start_time} seconds\")\n",
        "\n",
        "Expected Results:\n",
        "You will observe that the single-threaded approach will take the longest time.\n",
        "The multi-threaded and multiprocessing approaches will significantly reduce the time taken to load the data, with multiprocessing likely being the fastest if your system has multiple CPU cores.\n"
      ]
    }
  ]
}