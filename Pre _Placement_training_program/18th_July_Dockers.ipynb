{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZNvhLshB-Xw"
      },
      "outputs": [],
      "source": [
        "TOPIC: Docker\n",
        "1. Scenario: You are building a microservices-based application using Docker. Design a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. Ensure that the containers can communicate with each other properly.\n",
        "Design a Docker Compose file that sets up three containers: a web server, database, and cache. These containers need to communicate with each other properly.\n",
        "\n",
        "version: '3.8'\n",
        "\n",
        "services:\n",
        "  web:\n",
        "    image: nginx:latest\n",
        "    ports:\n",
        "      - \"8080:80\"\n",
        "    depends_on:\n",
        "      - db\n",
        "      - cache\n",
        "    networks:\n",
        "      - app-network\n",
        "\n",
        "  db:\n",
        "    image: mysql:5.7\n",
        "    environment:\n",
        "      MYSQL_ROOT_PASSWORD: rootpassword\n",
        "      MYSQL_DATABASE: myapp_db\n",
        "      MYSQL_USER: user\n",
        "      MYSQL_PASSWORD: password\n",
        "    ports:\n",
        "      - \"3306:3306\"\n",
        "    networks:\n",
        "      - app-network\n",
        "\n",
        "  cache:\n",
        "    image: redis:latest\n",
        "    ports:\n",
        "      - \"6379:6379\"\n",
        "    networks:\n",
        "      - app-network\n",
        "\n",
        "networks:\n",
        "  app-network:\n",
        "    driver: bridge\n",
        "\n",
        "The web server (NGINX), database (MySQL), and cache (Redis) containers are part of the same network app-network, allowing them to communicate internally.\n",
        "The depends_on option ensures that the web container starts after the database and cache containers are ready\n",
        "\n",
        "2. Scenario: You want to scale your Docker containers dynamically based on the incoming traffic. Write a Python script that utilizes Docker SDK to monitor the CPU usage of a container and automatically scales the number of replicas based on a threshold.\n",
        "\n",
        "Write a Python script that uses the Docker SDK to monitor CPU usage and automatically scale replicas based on a threshold.\n",
        "\n",
        "import docker\n",
        "import time\n",
        "\n",
        "# Initialize Docker client\n",
        "client = docker.from_env()\n",
        "\n",
        "def get_container_cpu_usage(container_name):\n",
        "    container = client.containers.get(container_name)\n",
        "    stats = container.stats(stream=False)\n",
        "    cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']\n",
        "    system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']\n",
        "    num_cpus = len(stats['cpu_stats']['cpu_usage']['percpu_usage'])\n",
        "    if system_delta > 0.0 and cpu_delta > 0.0:\n",
        "        cpu_percentage = (cpu_delta / system_delta) * num_cpus * 100.0\n",
        "        return cpu_percentage\n",
        "    return 0.0\n",
        "\n",
        "def scale_service(service_name, replicas):\n",
        "    service = client.services.get(service_name)\n",
        "    current_replica_count = service.attrs['Spec']['Mode']['Replicated']['Replicas']\n",
        "    if current_replica_count != replicas:\n",
        "        print(f\"Scaling service {service_name} to {replicas} replicas...\")\n",
        "        service.update(mode={'Replicated': {'Replicas': replicas}})\n",
        "\n",
        "def monitor_and_scale(container_name, service_name, threshold, max_replicas):\n",
        "    while True:\n",
        "        cpu_usage = get_container_cpu_usage(container_name)\n",
        "        print(f\"CPU usage for {container_name}: {cpu_usage:.2f}%\")\n",
        "\n",
        "        if cpu_usage > threshold:\n",
        "            scale_service(service_name, min(max_replicas, current_replicas + 1))\n",
        "        elif cpu_usage < threshold / 2 and current_replicas > 1:\n",
        "            scale_service(service_name, max(1, current_replicas - 1))\n",
        "\n",
        "        time.sleep(10)\n",
        "\n",
        "# Set the container to monitor and service to scale\n",
        "container_name = \"web_container\"\n",
        "service_name = \"web_service\"\n",
        "cpu_threshold = 70.0\n",
        "max_replicas = 10\n",
        "\n",
        "# Monitor and scale\n",
        "monitor_and_scale(container_name, service_name, cpu_threshold, max_replicas)\n",
        "\n",
        "Python Script for Docker Auto-scaling:\n",
        "\n",
        "import docker\n",
        "import time\n",
        "\n",
        "# Initialize Docker client\n",
        "client = docker.from_env()\n",
        "\n",
        "def get_container_cpu_usage(container_name):\n",
        "    container = client.containers.get(container_name)\n",
        "    stats = container.stats(stream=False)\n",
        "    cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']\n",
        "    system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']\n",
        "    num_cpus = len(stats['cpu_stats']['cpu_usage']['percpu_usage'])\n",
        "    if system_delta > 0.0 and cpu_delta > 0.0:\n",
        "        cpu_percentage = (cpu_delta / system_delta) * num_cpus * 100.0\n",
        "        return cpu_percentage\n",
        "    return 0.0\n",
        "\n",
        "def scale_service(service_name, replicas):\n",
        "    service = client.services.get(service_name)\n",
        "    current_replica_count = service.attrs['Spec']['Mode']['Replicated']['Replicas']\n",
        "    if current_replica_count != replicas:\n",
        "        print(f\"Scaling service {service_name} to {replicas} replicas...\")\n",
        "        service.update(mode={'Replicated': {'Replicas': replicas}})\n",
        "\n",
        "def monitor_and_scale(container_name, service_name, threshold, max_replicas):\n",
        "    while True:\n",
        "        cpu_usage = get_container_cpu_usage(container_name)\n",
        "        print(f\"CPU usage for {container_name}: {cpu_usage:.2f}%\")\n",
        "\n",
        "        if cpu_usage > threshold:\n",
        "            scale_service(service_name, min(max_replicas, current_replicas + 1))\n",
        "        elif cpu_usage < threshold / 2 and current_replicas > 1:\n",
        "            scale_service(service_name, max(1, current_replicas - 1))\n",
        "\n",
        "        time.sleep(10)\n",
        "\n",
        "# Set the container to monitor and service to scale\n",
        "container_name = \"web_container\"\n",
        "service_name = \"web_service\"\n",
        "cpu_threshold = 70.0\n",
        "max_replicas = 10\n",
        "\n",
        "# Monitor and scale\n",
        "monitor_and_scale(container_name, service_name, cpu_threshold, max_replicas)\n",
        "\n",
        "This script monitors the CPU usage of a container and scales a Docker service (via Docker Swarm) if usage exceeds a certain threshold.\n",
        "\n",
        "3. Scenario: You have a Docker image stored on a private registry. Develop a script in Bash that authenticates with the registry, pulls the latest version of the image, and runs a container based on that image.\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "# Docker registry credentials\n",
        "REGISTRY_URL=\"myregistry.com\"\n",
        "USERNAME=\"myusername\"\n",
        "PASSWORD=\"mypassword\"\n",
        "IMAGE_NAME=\"myregistry.com/myimage:latest\"\n",
        "\n",
        "# Authenticate with the private Docker registry\n",
        "echo \"Authenticating with Docker registry...\"\n",
        "docker login $REGISTRY_URL -u $USERNAME -p $PASSWORD\n",
        "\n",
        "if [ $? -ne 0 ]; then\n",
        "  echo \"Failed to authenticate!\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "# Pull the latest version of the image\n",
        "echo \"Pulling the latest image...\"\n",
        "docker pull $IMAGE_NAME\n",
        "\n",
        "# Run a new container based on the image\n",
        "echo \"Running the container...\"\n",
        "docker run -d --name mycontainer $IMAGE_NAME\n",
        "\n",
        "This script logs in to a private Docker registry, pulls the latest version of the specified image, and runs a container\n",
        "\n",
        "TOPIC: Airflow\n",
        "1. Scenario: You have a data pipeline that requires executing a shell command as part of a task. Create an Airflow DAG that includes a BashOperator to execute a specific shell command.\n",
        "Airflow DAG with BashOperator:\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "}\n",
        "\n",
        "with DAG('bash_operator_example', default_args=default_args, schedule_interval='@daily') as dag:\n",
        "    run_shell_command = BashOperator(\n",
        "        task_id='run_shell_command',\n",
        "        bash_command='echo \"Running shell command in Airflow!\"'\n",
        "    )\n",
        "\n",
        "This DAG runs a simple shell command using the BashOperator.\n",
        "\n",
        "2. Scenario: You want to create dynamic tasks in Airflow based on a list of inputs. Design an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list.\n",
        "\n",
        "Dynamic Task Creation in Airflow DAG:\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "}\n",
        "\n",
        "def process_data(item):\n",
        "    print(f\"Processing {item}\")\n",
        "\n",
        "input_list = ['item1', 'item2', 'item3']\n",
        "\n",
        "with DAG('dynamic_task_example', default_args=default_args, schedule_interval='@daily') as dag:\n",
        "    for item in input_list:\n",
        "        task = PythonOperator(\n",
        "            task_id=f'process_{item}',\n",
        "            python_callable=process_data,\n",
        "            op_args=[item]\n",
        "        )\n",
        "This DAG creates a task for each element in input_list using the PythonOperator.\n",
        "\n",
        "3. Scenario: You need to set up a complex task dependency in Airflow, where Task B should start only if Task A has successfully completed. Implement this dependency using the \"TriggerDagRunOperator\" in Airflow.\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "}\n",
        "\n",
        "with DAG('trigger_dagrun_example', default_args=default_args, schedule_interval='@daily') as dag:\n",
        "    task_a = DummyOperator(task_id='task_a')\n",
        "\n",
        "    trigger_b = TriggerDagRunOperator(\n",
        "        task_id='trigger_task_b',\n",
        "        trigger_dag_id='target_dag',\n",
        "    )\n",
        "\n",
        "    task_a >> trigger_b\n",
        "\n",
        "    This DAG triggers another DAG (target_dag) after task_a completes.\n",
        "\n",
        "TOPIC: Sqoop\n",
        "1. Scenario: You want to import data from an Oracle database into Hadoop using Sqoop, but you only need to import specific columns from a specific table. Write a Sqoop command that performs the import, including the necessary arguments for column selection and table mapping.\n",
        "sqoop import \\\n",
        "  --connect jdbc:oracle:thin:@//your_oracle_server:1521/your_db \\\n",
        "  --username your_username \\\n",
        "  --password your_password \\\n",
        "  --table your_table_name \\\n",
        "  --columns \"column1,column2,column3\" \\\n",
        "  --target-dir /user/hadoop/your_output_dir\n",
        "\n",
        "2. Scenario: You have a requirement to perform an incremental import of data from a MySQL database into Hadoop using Sqoop. Design a Sqoop command that imports only the new or updated records since the last import.\n",
        "\n",
        "sqoop import \\\n",
        "  --connect jdbc:mysql://your_mysql_server:3306/your_db \\\n",
        "  --username your_username \\\n",
        "  --password your_password \\\n",
        "  --table your_table_name \\\n",
        "  --incremental append \\\n",
        "  --check-column last_modified_column \\\n",
        "  --last-value '2023-01-01' \\\n",
        "  --target-dir /user/hadoop/your_output_dir\n",
        "\n",
        "\n",
        "3. Scenario: You need to export data from Hadoop to a Microsoft SQL Server database using Sqoop. Develop a Sqoop command that exports the data, considering factors like database connection details, table mapping, and appropriate data types.\n",
        "\n",
        "sqoop export \\\n",
        "  --connect jdbc:sqlserver://your_sql_server:1433;databaseName=your_db \\\n",
        "  --username your_username \\\n",
        "  --password your_password \\\n",
        "  --table your_table_name \\\n",
        "  --export-dir /user/hadoop/your_hadoop_data \\\n",
        "  --input-fields-terminated-by ',' \\\n",
        "  --update-mode allowinsert\n",
        "\n",
        "These Sqoop commands handle various data integration scenarios, from Oracle to MySQL and SQL Server."
      ]
    }
  ]
}