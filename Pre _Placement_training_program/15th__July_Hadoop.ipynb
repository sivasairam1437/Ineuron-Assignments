{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b14VwuxNirSm"
      },
      "outputs": [],
      "source": [
        "\n",
        "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def read_hadoop_config(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    components = {}\n",
        "\n",
        "    for property in root.findall('property'):\n",
        "        name = property.find('name').text\n",
        "        value = property.find('value').text\n",
        "        components[name] = value\n",
        "\n",
        "    core_components = {\n",
        "        \"fs.defaultFS\": components.get(\"fs.defaultFS\"),\n",
        "        \"dfs.replication\": components.get(\"dfs.replication\"),\n",
        "        \"yarn.resourcemanager.address\": components.get(\"yarn.resourcemanager.address\")\n",
        "    }\n",
        "\n",
        "    print(\"Core Components of Hadoop Configuration:\")\n",
        "    for key, value in core_components.items():\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
        "from hdfs import InsecureClient\n",
        "\n",
        "def calculate_total_file_size(hdfs_url, directory):\n",
        "    client = InsecureClient(hdfs_url)\n",
        "    status = client.status(directory, strict=False)\n",
        "    total_size = 0\n",
        "\n",
        "    if status:\n",
        "        files = client.list(directory, status=True)\n",
        "        for file_info in files:\n",
        "            total_size += file_info['length']\n",
        "\n",
        "    return total_size\n",
        "\n",
        "# Example usage:\n",
        "# hdfs_url = 'http://localhost:50070'\n",
        "# directory = '/user/hadoop/data'\n",
        "# print(f\"Total file size: {calculate_total_file_size(hdfs_url, directory)} bytes\")"
      ],
      "metadata": {
        "id": "KvCtAwysj6PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
        "from collections import Counter\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordFreqCount(MRJob):\n",
        "    def mapper(self, _, line):\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def reducer(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "def top_n_words(input_file, n):\n",
        "    job = MRWordFreqCount(args=[input_file])\n",
        "    word_counts = Counter()\n",
        "\n",
        "    with job.make_runner() as runner:\n",
        "        runner.run()\n",
        "        for word, count in job.parse_output(runner.cat_output()):\n",
        "            word_counts[word] += count\n",
        "\n",
        "    top_n = word_counts.most_common(n)\n",
        "    for word, freq in top_n:\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "# Example usage:\n",
        "# top_n_words('large_text_file.txt', 10)"
      ],
      "metadata": {
        "id": "x-jauQ56j6Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def check_hadoop_health_status(namenode_url):\n",
        "    response = requests.get(f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus')\n",
        "    health_info = response.json()\n",
        "\n",
        "    health_status = health_info['beans'][0]['State']\n",
        "    print(f\"NameNode Health Status: {health_status}\")\n",
        "\n",
        "    datanode_response = requests.get(f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem')\n",
        "    datanode_info = datanode_response.json()\n",
        "\n",
        "    live_datanodes = datanode_info['beans'][0]['NumLiveDataNodes']\n",
        "    dead_datanodes = datanode_info['beans'][0]['NumDeadDataNodes']\n",
        "\n",
        "    print(f\"Live DataNodes: {live_datanodes}\")\n",
        "    print(f\"Dead DataNodes: {dead_datanodes}\")\n",
        "\n",
        "# Example usage:\n",
        "# check_hadoop_health_status('http://localhost:50070')"
      ],
      "metadata": {
        "id": "bqZsdCU2j6V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
        "from hdfs import InsecureClient\n",
        "\n",
        "def list_hdfs_directory(hdfs_url, directory):\n",
        "    client = InsecureClient(hdfs_url)\n",
        "    files = client.list(directory)\n",
        "    print(f\"Files and directories in '{directory}':\")\n",
        "    for file in files:\n",
        "        print(file)\n",
        "\n",
        "# Example usage:\n",
        "# list_hdfs_directory('http://localhost:50070', '/user/hadoop')"
      ],
      "metadata": {
        "id": "Z1B4-K8ej6ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
        "import requests\n",
        "\n",
        "def analyze_storage_utilization(namenode_url):\n",
        "    response = requests.get(f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo')\n",
        "    datanodes_info = response.json()['beans'][0]['LiveNodes']\n",
        "    datanodes = json.loads(datanodes_info)\n",
        "\n",
        "    storage_info = []\n",
        "\n",
        "    for node, details in datanodes.items():\n",
        "        used = details['usedSpace']\n",
        "        capacity = details['capacity']\n",
        "        storage_info.append((node, used, capacity))\n",
        "\n",
        "    storage_info.sort(key=lambda x: x[1]/x[2], reverse=True)\n",
        "\n",
        "    print(\"DataNodes with highest storage utilization:\")\n",
        "    print(storage_info[0])\n",
        "\n",
        "    print(\"DataNodes with lowest storage utilization:\")\n",
        "    print(storage_info[-1])\n",
        "\n",
        "# Example usage:\n",
        "# analyze_storage_utilization('http://localhost:50070')"
      ],
      "metadata": {
        "id": "KRgZmezlj6cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
        "import requests\n",
        "\n",
        "def submit_hadoop_job(yarn_url, job_details):\n",
        "    response = requests.post(f'{yarn_url}/ws/v1/cluster/apps', json=job_details)\n",
        "    app_id = response.json()['app']['id']\n",
        "    print(f\"Job submitted with Application ID: {app_id}\")\n",
        "\n",
        "    # Monitor progress\n",
        "    while True:\n",
        "        status_response = requests.get(f'{yarn_url}/ws/v1/cluster/apps/{app_id}')\n",
        "        status = status_response.json()\n",
        "        print(f\"Job {app_id} Status: {status['app']['state']}\")\n",
        "        if status['app']['state'] in ['FINISHED', 'FAILED', 'KILLED']:\n",
        "            break\n",
        "\n",
        "    # Retrieve output\n",
        "    if status['app']['finalStatus'] == 'SUCCEEDED':\n",
        "        print(f\"Job {app_id} completed successfully\")\n",
        "        # Retrieve output logic can be implemented here"
      ],
      "metadata": {
        "id": "TE__7d_Zj6fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
        "import requests\n",
        "\n",
        "def submit_hadoop_job_with_resources(yarn_url, job_details, resource_requirements):\n",
        "    job_details['resource'] = resource_requirements\n",
        "    response = requests.post(f'{yarn_url}/ws/v1/cluster/apps', json=job_details)\n",
        "    app_id = response.json()['app']['id']\n",
        "    print(f\"Job submitted with Application ID: {app_id}\")\n",
        "\n",
        "    # Monitor resource usage and progress\n",
        "    while True:\n",
        "        status_response = requests.get(f'{yarn_url}/ws/v1/cluster/apps/{app_id}')\n",
        "        status = status_response.json()\n",
        "        print(f\"Job {app_id} Status: {status['app']['state']}\")\n",
        "        if status['app']['state'] in ['FINISHED', 'FAILED', 'KILLED']:\n",
        "            break\n",
        "\n",
        "        resource_usage = status['app']['resourceUsageReport']\n",
        "        print(f\"Resource Usage: {resource_usage}\")\n",
        "\n",
        "    # Retrieve output\n",
        "    if status['app']['finalStatus'] == 'SUCCEEDED':\n",
        "        print(f\"Job {app_id} completed successfully\")\n",
        "\n",
        "# Example usage:\n",
        "# resource_requirements = {\"vCores\": 4, \"memory\": 4096}\n",
        "# submit_hadoop_job_with_resources('http://localhost:8088', job_details, resource_requirements)"
      ],
      "metadata": {
        "id": "HZUbbrB4pq94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}