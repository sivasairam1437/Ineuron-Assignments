{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb4Nv-r0oDSh"
      },
      "outputs": [],
      "source": [
        "1. Working with RDDs:\n",
        "   a) Write a Python program to create an RDD from a local data source.\n",
        "   from pyspark import SparkContext\n",
        "\n",
        "def create_rdd_from_local():\n",
        "    sc = SparkContext(\"local\", \"RDD Example\")\n",
        "    data = [\"Spark\", \"Python\", \"RDD\", \"Example\"]\n",
        "    rdd = sc.parallelize(data)\n",
        "    return rdd\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rdd = create_rdd_from_local()\n",
        "    print(rdd.collect())\n",
        "\n",
        "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
        "   def process_rdd(rdd):\n",
        "    # Transformations\n",
        "    rdd_upper = rdd.map(lambda x: x.upper())\n",
        "    rdd_filtered = rdd_upper.filter(lambda x: \"A\" in x)\n",
        "\n",
        "    # Actions\n",
        "    result = rdd_filtered.collect()\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rdd = create_rdd_from_local()\n",
        "    processed_data = process_rdd(rdd)\n",
        "    print(processed_data)\n",
        "\n",
        "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
        "   def analyze_rdd(rdd):\n",
        "    # Mapping each word to a tuple (word, 1)\n",
        "    word_counts = rdd.map(lambda word: (word, 1))\n",
        "\n",
        "    # Reducing by key to count the occurrences of each word\n",
        "    reduced_word_counts = word_counts.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    return reduced_word_counts.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rdd = create_rdd_from_local()\n",
        "    word_counts = analyze_rdd(rdd)\n",
        "    print(word_counts)\n",
        "\n",
        "2. Spark DataFrame Operations:\n",
        "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def load_csv_to_dataframe(file_path):\n",
        "    spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_csv_to_dataframe(\"data.csv\")\n",
        "    df.show()\n",
        "\n",
        "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
        "\n",
        "   def dataframe_operations(df):\n",
        "    # Filtering\n",
        "    filtered_df = df.filter(df['age'] > 30)\n",
        "\n",
        "    # Grouping\n",
        "    grouped_df = df.groupBy('department').count()\n",
        "\n",
        "    # Joining (assuming another DataFrame df2)\n",
        "    df2 = df.withColumnRenamed('id', 'employee_id')  # Example DataFrame\n",
        "    joined_df = df.join(df2, df['id'] == df2['employee_id'])\n",
        "\n",
        "    return filtered_df, grouped_df, joined_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_csv_to_dataframe(\"data.csv\")\n",
        "    filtered_df, grouped_df, joined_df = dataframe_operations(df)\n",
        "\n",
        "    filtered_df.show()\n",
        "    grouped_df.show()\n",
        "    joined_df.show()\n",
        "\n",
        "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
        "\n",
        "   def apply_spark_sql(df):\n",
        "    df.createOrReplaceTempView(\"employees\")\n",
        "    spark = df.sparkSession\n",
        "\n",
        "    # Example SQL query\n",
        "    result_df = spark.sql(\"SELECT department, AVG(salary) as avg_salary FROM employees GROUP BY department\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_csv_to_dataframe(\"data.csv\")\n",
        "    result_df = apply_spark_sql(df)\n",
        "    result_df.show()\n",
        "\n",
        "3. Spark Streaming:\n",
        "  a) Write a Python program to create a Spark Streaming application.\n",
        "\n",
        "  from pyspark.streaming import StreamingContext\n",
        "\n",
        "def create_spark_streaming_app():\n",
        "    sc = SparkContext(\"local[2]\", \"Streaming Example\")\n",
        "    ssc = StreamingContext(sc, 1)  # 1-second batch interval\n",
        "    return ssc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ssc = create_spark_streaming_app()\n",
        "    ssc.start()\n",
        "    ssc.awaitTermination()\n",
        "\n",
        "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
        "\n",
        "   def consume_data_from_socket(ssc, hostname, port):\n",
        "    lines = ssc.socketTextStream(hostname, port)\n",
        "    return lines\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ssc = create_spark_streaming_app()\n",
        "    lines = consume_data_from_socket(ssc, \"localhost\", 9999)\n",
        "    lines.pprint()\n",
        "\n",
        "    ssc.start()\n",
        "    ssc.awaitTermination()\n",
        "\n",
        "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
        "\n",
        "   def process_stream(lines):\n",
        "    words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "    word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "    word_counts.pprint()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ssc = create_spark_streaming_app()\n",
        "    lines = consume_data_from_socket(ssc, \"localhost\", 9999)\n",
        "    process_stream(lines)\n",
        "\n",
        "    ssc.start()\n",
        "    ssc.awaitTermination()\n",
        "\n",
        "\n",
        "4. Spark SQL and Data Source Integration:\n",
        "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
        "\n",
        "   def connect_to_database():\n",
        "    spark = SparkSession.builder.appName(\"Database Integration\").getOrCreate()\n",
        "    jdbc_url = \"jdbc:mysql://localhost:3306/db_name\"\n",
        "    properties = {\n",
        "        \"user\": \"username\",\n",
        "        \"password\": \"password\",\n",
        "        \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "    }\n",
        "    df = spark.read.jdbc(jdbc_url, \"table_name\", properties=properties)\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = connect_to_database()\n",
        "    df.show()\n",
        "\n",
        "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
        "\n",
        "def perform_sql_operations(df):\n",
        "    df.createOrReplaceTempView(\"table_name\")\n",
        "    spark = df.sparkSession\n",
        "\n",
        "    result_df = spark.sql(\"SELECT * FROM table_name WHERE age > 30\")\n",
        "    return result_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = connect_to_database()\n",
        "    result_df = perform_sql_operations(df)\n",
        "    result_df.show()\n",
        "\n",
        "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n",
        "\n",
        "   def load_data_from_hdfs():\n",
        "    spark = SparkSession.builder.appName(\"HDFS Integration\").getOrCreate()\n",
        "    hdfs_path = \"hdfs://localhost:9000/user/hadoop/data.csv\"\n",
        "    df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
        "    return df\n",
        "\n",
        "def load_data_from_s3():\n",
        "    spark = SparkSession.builder.appName(\"S3 Integration\").getOrCreate()\n",
        "    s3_path = \"s3a://my-bucket/data.csv\"\n",
        "    df = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_hdfs = load_data_from_hdfs()\n",
        "    df_hdfs.show()\n",
        "\n",
        "    df_s3 = load_data_from_s3()\n",
        "    df_s3.show()\n"
      ]
    }
  ]
}